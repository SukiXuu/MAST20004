# Topic02\_Conditional\_Probability\_and\_Independence

 \#  Topic02 Conditional\_Probability\_and\_Independence \[TOC\]

## 第4课：条件概率，事件的独立性

在许多情况下，当我们试图了解事件的概率时，通常会给穿戴者一些部分信息. 附加信息的存在通常会影响我们对事件可能性的判断. 我们可以通过以下简单示例来说明这一点.

假设有两个，第一个包含5个红色球，第二个包含5个绿色球. 我们从十个球池中随机选择一个球. 没有任何信息， 我们知道选择红球的概率为1/2. 现在假设给了我们额外的信息，那就是选定的球是从第一个中出来的. **在此新信息下**, 获得红色球的可能性变为1，因为第一个中的球都是红色！这个简单的极端例子已经说明了附加信息的存在如何改变对概率的判断. 为了使这一点在数学上更加精确，我们将引导对条件概率的研究. 条件化技术在现代概率论中尤为重要，例如在学习 Markov processes andmartingales 的时候

✎: 附加信息导致了概率的变化-&gt;条件概率的初步形成

### 1 两个说明性示例

ể.ĝ.1.1 投掷两个骰子. 样本空间由Ω= {（1,1），（1,2）···，（6,6）}给出，其中包含36个元素. 考虑“两个骰子的和为8”的事件A，即，A = {（2,6），_（3,5）_，（4,4），（5,3），（6,2）}. 由于这是一个经典的概率模型，因此我们看到 `P（A）=＃A/＃Ω= 5/36` 从数学上讲，此信息由另一个事件 H = {（3,1），（3,2），（3,3），（3,4），_（3,5）_，（3,6）}总结. 给定此新信息（即给定H的存在）的安倍概率？关键是，在此新信息的作用下，我们的样本空间已更改为H，因为唯一相关的结果是第一个骰子对象为3. 显然，所有这些结果均应发生的可能性相同. 另外，触发事件A的H的六个结果中恰好有一个结果，即 _（3,5）_. “Conditional Probability”of A given H is 1/6. 这与我们刚刚计算出的无条件概率不同.

ể.ĝ.1.2. 在经典概率模型的背景下，可以对上述示例进行概括，以提供更具说明性的讨论. 设Ω为有限集，然后考虑经典概率模型的环境. LetA，H为两个事件. 我们知道A的“无条件概率”由P（A）=＃A/＃Ω给出. 要定义A \(given H\) 的“条件概率”，就像例1.1一样，我们现在限制“新”样本空间H. 在H的所有结果中，触发A的是事件A∩H中的那些. 因此，在经典概率模型下，自然可以预期，给定H的“条件概率”应定义为

℅ 条件概率公式：（已知H事件发生为前提） "conditional probability" of A given H should be defined by: `P(A|H) =#(A∩H)/#H=#[(A∩H)/#Ω]/[#H/#Ω]=P(A∩H)/P(H)`

我们可以很容易地检查出，这正是示例1.1中所发生的，即该示例中的P（A∩H）P（H）= \[1/36\]/\[1/6\]=1/6.

### 2 条件概率的定义

前面的讨论可以说超越经典概率模型，使我们引出了条件概率的一般定义. 总体思路： 假设A和H是两个事件. 假设我们总共进行了n次随机试验（n 很大）. 设n1 为H发生次数，设n2为A∩H发生次数. 显然，n2/n1给出了发生这些实验的次数的比例.

> heuristic principle that “relative frequencies should stabilise at thetheoretical probability as n→∞”,

基于启发式原理，“当n→∞时相对频率应稳定在理论概率”，我们可以得到： `n2/n1近似于“理论条件概率P（A | H）`， 当n→∞，由于相同的原理，我们知道 `n2/n→P（A∩H），n1/n→P（H）as n→∞` 因此，我们得到`n2/n1 = [n2 / n]/[n1 / n]→P（A∩ H）P（H）as n→∞` 因此，将P（A∩H）P（H）表达式作为给出H发生的条件概率的定义是合理的. 当然，这仅在P（H）&gt; 0时才有意义.

#### Definition 2.1 Conditional Prabability

Let A和H为两个事件，并假定P（H）&gt; 0. 假设H发生的条件概率由`P（A | H）= P（A∩H）/P（H）`定义

℅ 2.1 定力的逆用 使用P（A \| H）的定义，我们有P（A∩H）= P（H）·P（A \| H）. 这通常称为乘法定理 multiplication theorem

ể.ĝ.2.1. （i）抛骰子. 考虑事件A = {2}，H = {2,4,6}. 然后P（A）= 16，P（A \| H）= P（A∩H）P（H）= 1/61/2 = 13 .

（ii）扔一对骰子. 考虑事件A = {（i，j）：\| i−j \| = 1}，H = {（i，j）：i + j = 7}. 看一下P（A ）= 518，P（A \| H）= 13.

在上一个示例的（i）和（ii）中，我们看到P（A \| H）&gt; P（A）. 换句话说，发生H会增加发生A happens的机会. 在这种情况下，

℅ Positive relation and Negative relation 我们说A和H之间存在正关系 Positive relation. 由于 `P(A|H)>P(A)⇐⇒P(A∩H)P(H)>P(A)⇐⇒P(A∩H)P(A)>P(H)⇐⇒P(H|A)>P(H)` 请注意，相应地`P(A|H)<P(A)`表示A H成负相关 negative relation.

ể.ĝ.2.2 您要去面试一份工作. 共有7个候选人，并将提供3个职位. 在这种情况下，这七个候选人中的三个人（收到要约的人）给出了通用结果. 样本空间总共包含（73）= 35个结果. 假设在这7个候选人中，您和X女士具有工作所需的特定技能. 因此，你们中的一个很可能会被雇用，但你们两个都不会获得聘用. 结果是不合理的

用经典概率模型Ω（ể.ĝ.即假设所有结果均等概率出现）工作. 取而代之的是，让我们假设，潜在的概率函数（先验概率）是通过以下方式给出的. （i）与“您和X女士都被雇用”的事件相对应的每个结果都有概率160. 简单计数表明，此事件中有（5å1）= 5个结果. 因此，此事件的概率为5/60. （ii）与“您找到工作但X女士没有”的事件相对应的每个结果都有概率1/24. 此事件中有（5å2）= 10个元素，因此该事件的概率为10/24. （iii）对应于“您没有找到工作，但X女士确实有”的事件的每个结果都具有概率124. 此事件中有（5å2）= 10个元素，因此该事件的概率为10/24. （iv）与“您或X女士均未找到工作”事件相对应的每个结果的概率为1/120. 此事件有（53）= 10个元素，因此该事件的概率为10/120.很显然，以上规范定义了一个合法概率函数. 假设没有雇用X女士，您雇用的条件概率是多少？ 引入以下事件：A：您被雇用，H：MsX. desired 条件概率为P（A \| Hc）. 根据underlying probability定义，我们有

∆ P\(A∩Hc\) =1024=512\(Case \(ii\)\)  
 ∆ P\(Hc\) =P\(A∩Hc\) +P\(Ac∩Hc\) \(Case \(ii\) + Case \(iv\)\)  
 =10/24+10/120=1/2.

Therefore, ∆ P\(A\|Hc\) =P\(A∩Hc\)/P\(Hc\)=\[5/12\]/\[1/2\]=5/6

我们还可以计算出 ∆ P（A）= P（A∩H）+ P（A∩Hc）（情况（i）+情况（ii））  
 = 5/60 + 10/24 = 60/120 = 1/2.  
∆ X没有找到工作，您的机会大大增加了. 但这从先验假设来看并不太令人惊讶.

### 3事件的独立性 Independence of events

我们已经看到了两个事件之间正/负关系的概念. 如果P（A \| H）= P（A）会怎样？ 这就是说，H的出现并不影响A的概率. 换句话说，这表明这两个事件之间存在某种“独立性”. 回想一下，属性P（A \| H）= P（A）在数学上等价于 `P（A∩H）= P（A）·P（H）`

#### Definition 3.1.

两个事件A，裸称独立，if P（A∩ B）= P（A）·P（B）. Two events are said to bedependentif they are not independent

ể.ĝ.3.1 掷骰子. 考虑以下事件：A = {2,4,6}，B = {1,2,3}，C = {1,2,3,4}. 我们有P（A）= P（B）= 12， P（C）= 23，P（A∩B）= 16，P（A∩C）= 13. 因此，P（A∩B）6 = P（A）·P（B），P（A∩C ）= P（A）·P（C）. 换句话说，A与B dependent，而A与C Independent.

℅ 3.1 到目前为止，我们已经看到两个有关两个事件之间某种“不相关性”的概念：_独立性_和_不相交性_. 这两个概念完全不同. 一方面，测试脱节仅需要检查两个事件中的元素，以查看是否存在共同的元素. 此过程不涉及任何分配的概率. 另一方面，测试独立性必须包含潜在的概率函数，这从定义中可以明显看出. 但是仍然可以对他们之间的关系描述： 假设P（A）&gt; 0和P（B）&gt; 0. 如果A，B不相交，则 `0 = P（∅）= P（A∩B）< P（A）·P（B）`，因为右侧严格为正. 换句话说，A，B必须不是独立的. 更精确地，在这种情况下，A具有负关系：`0 = P（A | B）< P（A）` 这并不奇怪，因为B的出现排除了（不相交）发生的可能性.

℅ property  
如果A与B独立，则以下任意对也是如此： ∆ （i）AcandB， ∆ （ii）AandBc， ∆ （iii）AcandBc Proof. 我们仅验证（i），其余两个作为练习. 首先，我们进行分解 B =（A∩B）∪（Ac∩B） 得出 ∆ P（Ac∩B）= P（B）-P（A∩B）（通过有限可加性） = P（B） -P（A）·P（B）（通过A，B的独立性） =（1-P（A））·P（B） = P（Ac）·P（B）（通过概率函数的性质3） 因此，我们得出结论，Acand B是独立的.

#### 两个以上事件的独立性

接下来一个自然的问题是将独立性的概念扩展到两个以上事件的上下文中，例如A1，A2，···，An. 这种扩展的首次尝试是要求事件之间成对独立，即通过要求 Ai，Aj are独立于任意对i ≠ j. 我们用一个简单的例子来说明为什么这不足以说明这些事件之间的“完全独立性Total Independence”

ể.ĝ.3.2 扔出两个公平的硬币. 考虑以下三个事件： A：第一个硬币是Head， B：第二个硬币是Head， C：两个硬币中的一个恰好是Head. 简单计算表明 `P（A）= P（B）= P（C）= 12`，并且 `P（A∩B）= P（A∩C）= P（B∩C）= 14`. 结果，我们看到这三个事件是成对独立的. 但是，它们确实共享某种依赖性，如以下所示. 假设我们知道A∩B发生，即两个硬币都产生Head. 此信息立即排除了C的可能性. 换句话说，`P（C |A∩B）= 0 ≠ P（C）`. 这表明A∩BandC之间存在依赖性，从而揭示了这三个事件之间的一定依赖性. 上面的示例告诉我们，**成对独立不足以捕获“全部”独立”\[pairwise independent is not sufficient tocapture “total independenc\]**. 确实，我们需要验证更多的方程式以建立独立性.

#### Definition 3.2. 多个事件相对成立

LetA1，A2，···，An 为n个事件. 我们说A1，···，相互独立\[mutually independent,\]，如果对于这些事件的任何子集合，即对于任何子集合{j1，···，jm}⊆{1，···，n}，以下恒等成立true： `P（Aj1∩···∩Ajm）= P（Aj1）×··×P（Ajm）`.

第5课：两个以上事件的独立性，总概率定律，贝叶斯公式 1事件之间的相互独立性我们首先回顾以下定义。 定义1.1.LetA1，A2，···，An be n events。我们说A1，···，相互独立，如果对于这些事件的任何子集合，即对于任何子集合{j1，···，jm}⊆{1，···，n}，以下恒等成立true：`P（Aj1····Ajm）= P（Aj1）×··×P（Ajm）`。让我们用几个例子来看一下在定义相互独立性时需要多少个方程。 例子1.1。 （ i）如果只有两个事件A B involved，则只需要一个方程，恰好是两个事件的独立性定义。 （ii）如果有三个事件A，B和C涉及，我们需要满足以下所有方程式才能建立三个事件之间的相互独立性：两个事件的子集合：  P（A∩B）= P（A）·P（B）， P（A∩C）= P（A）·P （C）， P（B∩C）= P（B）·P（C）， 三个事件的子集合：P（A∩B∩C）= P（A）·P（B）·P（C） 换句话说，总共需要4个方程。 （iii）如果有四个事件A，B，C，D，我们需要以下所有等式： 两个事件的子集合： P（A∩ B）= P（A）·P（B）， P（A∩C）= P（A）·P（C）， P（A∩D）= P（A）·P（D）， P（B ∩C）= P（B ）·P（C）， P（B∩D）= P（B）·P（D）， P（C∩D）= P（C）·P（D）， P\(A∩B∩C\) =P\(A\)·P\(B\)·P\(C\),‘ P\(A∩B∩D\) =P\(A\)·P\(B\)·P\(D\), P\(A∩C∩D\) =P\(A\)·P\(C\)·P\(D\), P\(B∩C∩D\) =P\(B\)·P\(C\)·P\(D\), 和四个事件的子集合: P\(A∩B∩C∩D\) =P\(A\)·P\(B\)·P\(C\)·P\(D\). 总共有11个等式。 （iv）更普遍地讲，如果涉及事件，则一个人总共需要2n-1个等式（为什么？）来测试相互独立性！设A1，A2，···，是一个给定的相互独立的事件家族。我们可以从它们中形成许多相互独立的事件的新集合。 基本原理是从{A1，···，在非重叠路径中选择一个，并在每个子集合中执行任意操作。例如，以下事件家族中的每一个都是相互独立的： （i）A1∩A2，Ac3; （ii）A1∪A2，A3∪A4，A5，Ac7; （iii）A1∪A2，A3∩A4，（A7  A9）∪A15，让我们证明情况 （i）的独立性。证明，我们直接检查定义： P（（A1∩A2）∩Ac3）= P（A1∩A2）-P（A1∩A2∩A3 ） = P（A1）·P（A2）-P（A1）·P（A2）·P（A3） = P（A1）·P（A2）·（1-P（A3）） = P（A1∩ A2）·P（Ac3）， 其中第二个等式来自于相互独立性定义中的两个方程。 另一方面，正如我们强调的那样，以不重叠的方式形成这些集合非常重要，否则我们将破坏独立。 例如，事件A1∩A2和A2∩A3通常不是独立的（这些集合有一个A2（重叠）。实际上，如果它们是独立的，那么根据定义，我们将具有P（（A1∩A2）∩（A2∩A3））= P（A1∩A2）·P（A2∩A3）。（1.1） 假定A1，A2，A3为彼此独立，我们也知道（1.1）的左侧等于 P（A1∩A2∩A3）= P（A1）·P（A2）·P（A3）， 而（1.1）的右侧等于 P（A1） ·P（A2）·P（A2）·P（A3）= P（A1）·P（A2）2·P（A3） 结果，我们将获得以下标识： P（A1）·P（A2 ）·P（A3）= P（A1）·P（A2）2·P（A3） 但是，如果P（A1）&gt; 0，P（A3）&gt; 0, 0&lt; P\(A2\)&lt;1\(why?\) 1\(why?\),which is almost always the case for most examples.

备注1.1 作为相互独立定义中的一个特定方程，我们有 P（A1∩A2∩···∩An）= P（A1）···· ·P（An）。（1.2）但是，仅知道（1.2）不足以保证A1，···An的相互独立性。一个简单的例子是三个事件A，A，∅。我们有 P（A∩A∩∅）= P（∅）= 0 = P（A）·P（A）·P（∅）。 但是，这三个事件不是相互独立的，因为A不是自身独立的（除非 应用：系统可靠性独立性的概念在系统可靠性研究中有很好的应用。在简化的上下文中，我们可以将系统视为包含以下内容的电路：几个组件，以顺序和并行的方式连接。下图给出了一个系统的两个示例： 图1：系统可靠性。在给定的系统中，通常假定所有组件都独立运行，但是每个组件都有特定的故障概率。 如果组件无法运行，则电流无法流过该组件。当且仅当电流可以从起点（左）流向终点（右）时，系统才能正常运行。我们将系统的可靠性解释为系统正常运行的概率。让我们尝试了解图1中给出的两个系统中哪个更可靠。 我们假设每个组件具有0.1的相同故障概率。对于系统A，我们可以将其分为第一部分和第二部分。由于这两部分是顺序连接的，因此我们知道只有当两部分都正常工作时，系统才能正常运行（即电流从左向右流动）。 另外，我们假设所有组件都是独立工作的。因此，通过独立性，我们有 P（系统A函数）= P（第一部分函数）×P（第二部分函数） 现在，我们计算第一部分起作用的概率。由于第一部分中的两个组件是并联连接的，因此我们知道，当且仅当两个组件中的至少一个起作用时，第一部分才起作用（即，电流可以流过第一部分）。在这种情况下，考虑以下因素的互补是更简单的事件。即我们有P（第I部分功能） = 1-P（第I部分失败） = 1-P（组件1和2都失败） = 1-P（组件1失败）×P（组件2失败）（根据独立性） = 1 −0.1×0.1 = 0.99。 第二部分的结果相同。 因此，第一个系统的可靠性由 P（系统1函数）= 0.99×0.99 = 0.9801给出。 系统B的分析完全相似。我们直接写下公式： P（系统B函数）=（1-0.1）×（1-0.13）= 0.8991。 换句话说，系统A比系统B更可靠，这是娱乐性更一般的数学问题。假设我们手头有每个组件，每个组件都有失效概率p。使用这些组件，我们如何设计最可靠的系统？ 2总概率定律 有两个特别重要的工具来计算概率。 第一个被称为总概率定律，另一个被称为贝叶斯公式。 正如我们将看到的，这两个原理背后的哲学是非常相反的。我们首先讨论总概率定律。该法律背后的基本思想可以概括如下。我们认为事件是由于多种不同原因/原因之一而产生的结果/结果。通过这种方式，我们通过对这些可能性的可能原因之一进行调节来计算事件的概率。在给出精确的数学公式之前，我们首先需要引入分区的概念。定义给定的样本空间。 定义2.1。 如果事件{A1，A2，A3···}（可能是_有限集合或无限序列_）的集合称为Ω的分离互不相交且穷举， 即if `Ai∩Aj=∅`for alli6 =j and ∪n An =Ω  
下图给出了（的（有限）划分的一个示例： 图2：Ω的划​​分为了描述总概率定律，让给定一个事件，令{A1，A2，A3，···}是given的给定分区（有限或可数无限）。我们的目标是计算P（H）。为此，我们首先写 H =H∩Ω=H∩（∪nAn）（因为An是穷举性的） =∪n（H∩An）， 因为事件H∩An（对于不同的n）也通过不可数加和公理相互分离我们有 P（H）= ∑nP（H∩An） 通过进一步写出 P（H∩An）= P（An）·P（H \| An）， 我们得出了总概率定律，其描述如下。总概率。让H成为一个事件，让{A1，A2，···}成为Ω的分区。 假设每个n的P（An）&gt; 0（以便理解条件概率）。 然后我们可以通过以下公式计算P（H）： P（H）= ∑nP（An）·P（H \| An）。（2.1） 备注2.1。 如果分区是有限的，则（2.1）的右侧是有限和，如果分区是由一系列事件给定的，则它是无限和（即一系列）。在上面的讨论中，我们只写了下标（未指定其范围）允许这两种情况。如前所述，总概率定律可以解释如下。我们解释有一个效果，并将一个分区{A1，A2，···}解释为导致效果H的可能不同原因。因此，总概率定律允许我们首先对这些原因之一进行条件运算，然后再加上所有这些可能性（互不相交和穷举），以计算出P（H）。 示例2.1。 HIV检测具有90％的准确度，即如果一个人是HIV阳性，则有90％的机会将其检测为阳性。从某种意义上说，如果一个人不是艾滋病毒呈阳性，则测试有5％的错误，仍然有5％的机会将他/她测试为阳性。假设人口的0.01％是HIV阳性。解决方案。为回答这个问题，我们首先定义以下事件：H：随机选择的人被测试为阳性，A1：人为HIV阳性，A2：该人不为阳性。 HIV阳性。我们可以将事件H（测试阳性）看作是两个可能原因的结果：一个人确实是HIV阳性或一个人健康（但测试会出错）。这两个原因显然是_不相交和详尽_的。 因此，我们可以使用总概率定律： P（H）= P（A1）·P（H \| A1）+ P（A2）·P（H \| A2）= 0.0001·0.9 + 0.9999·0.05≈0.05.A更多对于前面的示例，有趣的问题如下。假设此人被检测为阳性，那么他/她实际上是HIV阳性的条件概率是多少？ 这个问题很重要，因为它可以清楚地表明测试的有效性。这个问题的本质是_在给定效果的情况下寻找特殊原因_。这使我们找到了另一个称为贝叶斯公式的重要工具。 3贝叶斯公式 我们继续采用与总概率定律相同的假设。即，他是给定的事件，{A1，A2，···}是Ω的给定分区。贝叶斯公式与计算某人的条件概率P（Ai \| H）有关。公式在数学上很容易建立：P（Ai \| H）= P（Ai∩H）P（H）（条件概率的定义）= P（Ai）·P（H \| Ai）∑jP（Aj）·P （H \| Aj）（将LOTP应用于分母）。 贝叶斯公式。 令H为一个事件，令{A1，A2，···}为Ω的给定分区。假设每个n的P（H）&gt; 0和P（An）&gt; 0。然后对于每个，我们可以通过以下公式计算P（Ai \| H）：P（Ai \| H）= P（Ai）·P（H \| Ai）∑jP（Aj）·P（H \| Aj）。数量P（Ai \| H）是“由于结果/原因而造成的影响/结果发生，这是有条件的概率”。

示例3.1。 在示例2.1中，让我们计算该人为HIV阳性的条件概率她被测试为阳性吗？这是由P（A1 \| H）= P（A1）·P（H \| A1）P（H）给出的。由于我们已经根据总概率定律计算了P（H），因此我们直接在这里代入数字：P（A1 \| H）= 0.0001×0.90.05 = 0.0018。换句话说，这表明即使检测结果为阳性，该人也很可能患有艾滋病毒。因此，该测试没有任何价值。这是一个令人惊讶的结果，因为乍一看这些假设，并没有暗示该测试是无用的。正是在执行了概率分析（即computingP（A1 \| H））之后，我们得出了这样的结论（数学批判性思维的一个很好的例子）。为了深入了解发生了什么，让我们回想方程 P（A1 \| H）=\[ P（A1）·P（H \| A1）\]/P\[（A1）·P（H \| A1）+ P（A1）·P（H \| A1）\] = 0.0001× 0.90.0001×0.9 + 0.9999×0.05 = 11 + 0.9999×0.050.0001×0.9 \(3.1\) 这里的主要问题是，0.0001（艾滋病毒感染率）极少，这当然是任何人的本性。罕见疾病。这导致以下事实：分母中的0.0001×0.9项远远小于0.9999×0.5项，导致表达式（3.1）的P（A1 \| H）较小。表达式（3.1）还表明，为了使测试有用，我们需要将数字0.05（错误率）小得多或将数字0.9（准确性）大得多。简而言之，为了有效，测试需要比其准确得多。

